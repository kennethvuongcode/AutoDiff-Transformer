Fused operators are optimized computational kernels which combine multiple operations into a single function with the benefit of improving performance and efficiency in a deep learning framework. 
When executed separately, each operation requires launching a kernel of a GPU, each with scheduling latency costs for synchronization. Fused operators allow for multiple computations to be computed together, therefore reducing the number of kernel launches, which optimizes for memory access and reduces overhead execution. 
The intuition behind it is that by generating fused operators for the most commonly seen pairs of operations, such as Matmul + Layernorm or Matmul + Softmax, we are accumulate many small optimizations to make a large impact on the performance of a framework.
The main contribution to the optimizations are the practical steps the machine takes when reading funcitons. By fusing operators, function call overhead is reduced, therefore reducing the amount of memory read (and written), as well a argument passing, and fewer intermediate 
tensors that must be made. 
Currenly my forward pass computes around 1x speedup and the backwards pass is around 0.8x, which is expected for smaller sized 2D tensors. This is expected, as the payoffs for memory reuse and reduced kernel launches
do not outweigh the cost of the extra work for having both operations in the same cache. This is expected to perform better on large datasets. As for optimization, there is more potential for improvement in computing the gradients. 
For the forward computation, most my operations are calls to torch implementations, which are highly optimized. An area of potential improvement with this current implementation of operations would be made to the gradients functions.
Tiling helps improve performance by maximizing paralleism through dividing large computations into smaller ones which are processed efficiently. We can leverage tiling to break up computations into smaller tiles for more memory-efficient processing. 